{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_data/ogb_arxiv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pgm/lib/python3.10/site-packages/torch/__init__.py:533\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    535\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All citations:\")\n",
    "for citation in data_edge_index.t().tolist():\n",
    "    print(citation)\n",
    "\n",
    "# Finding unique citations\n",
    "unique_citations = set(tuple(c) for c in data_edge_index.t().tolist())\n",
    "\n",
    "# Printing unique citations\n",
    "print(\"\\nUnique citations:\")\n",
    "for citation in unique_citations:\n",
    "    print(citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(f\"preprocessed_data/new/cora_random_sbert.pt\", map_location='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the data\n",
    "print(data, type(data))\n",
    "print(data.label_names)\n",
    "\n",
    "print(data.raw_text[0]) # raw text is title: abstract\n",
    "\n",
    "print(data.y[0]) # label\n",
    "print(data.y.unique())\n",
    "\n",
    "# print(data.edge_index[:,5]) # edge index\n",
    "print(data.train_masks) # train mask\n",
    "print(data.train_masks[0].shape)\n",
    "print(sum(data.train_masks[0]),sum(data.train_masks[6])) # train mask\n",
    "print(data.val_masks) # val mask\n",
    "print(data.test_masks) # test mask\n",
    "\n",
    "print(data.x[0].shape) # x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in range(data.edge_index.shape[1]):\n",
    "    curr = tuple(data.edge_index[:,i].numpy())\n",
    "    if curr in d:\n",
    "        d[curr] += 1\n",
    "    else:\n",
    "        d[curr] = 1\n",
    "unique_edges = set()\n",
    "for k in d.keys():\n",
    "    curr = tuple(sorted(list(k)))\n",
    "    unique_edges.add(curr)\n",
    "print(\"Number of unique edges\",len(unique_edges))\n",
    "print(set(d.values()))\n",
    "print(len(d))\n",
    "# most of the edges are repeated twice but it should be a directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(f\"preprocessed_data/new/cora_random_sbert.pt\", map_location='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 384], raw_texts=[2708], category_names=[2708]) <class 'torch_geometric.data.data.Data'>\n",
      "['Rule_Learning', 'Neural_Networks', 'Case_Based', 'Genetic_Algorithms', 'Theory', 'Reinforcement_Learning', 'Probabilistic_Methods']\n",
      " Stochastic pro-positionalization of non-determinate background knowledge. : It is a well-known fact that propositional learning algorithms require \"good\" features to perform well in practice. So a major step in data engineering for inductive learning is the construction of good features by domain experts. These features often represent properties of structured objects, where a property typically is the occurrence of a certain substructure having certain properties. To partly automate the process of \"feature engineering\", we devised an algorithm that searches for features which are defined by such substructures. The algorithm stochastically conducts a top-down search for first-order clauses, where each clause represents a binary feature. It differs from existing algorithms in that its search is not class-blind, and that it is capable of considering clauses (\"context\") of almost arbitrary length (size). Preliminary experiments are favorable, and support the view that this approach is promising.\n",
      "tensor(0)\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "[tensor([False,  True,  True,  ...,  True,  True,  True]), tensor([ True,  True,  True,  ...,  True, False,  True]), tensor([ True,  True, False,  ..., False,  True, False]), tensor([False,  True, False,  ...,  True,  True, False]), tensor([ True, False,  True,  ..., False, False,  True]), tensor([False,  True,  True,  ..., False,  True, False]), tensor([False,  True, False,  ..., False, False,  True]), tensor([ True, False,  True,  ...,  True,  True,  True]), tensor([ True,  True,  True,  ..., False, False,  True]), tensor([ True,  True, False,  ...,  True, False,  True])]\n",
      "torch.Size([2708])\n",
      "tensor(1624) tensor(1624)\n",
      "[tensor([ True, False, False,  ..., False, False, False]), tensor([False, False, False,  ..., False,  True, False]), tensor([False, False,  True,  ..., False, False,  True]), tensor([False, False, False,  ..., False, False, False]), tensor([False,  True, False,  ...,  True,  True, False]), tensor([False, False, False,  ..., False, False,  True]), tensor([False, False,  True,  ...,  True,  True, False]), tensor([False, False, False,  ..., False, False, False]), tensor([False, False, False,  ...,  True, False, False]), tensor([False, False, False,  ..., False,  True, False])]\n",
      "[tensor([False, False, False,  ..., False, False, False]), tensor([False, False, False,  ..., False, False, False]), tensor([False, False, False,  ...,  True, False, False]), tensor([ True, False,  True,  ..., False, False,  True]), tensor([False, False, False,  ..., False, False, False]), tensor([ True, False, False,  ...,  True, False, False]), tensor([ True, False, False,  ..., False, False, False]), tensor([False,  True, False,  ..., False, False, False]), tensor([False, False, False,  ..., False,  True, False]), tensor([False, False,  True,  ..., False, False, False])]\n",
      "torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "# analyse the data\n",
    "print(data, type(data))\n",
    "print(data.label_names)\n",
    "\n",
    "print(data.raw_text[0]) # raw text is title: abstract\n",
    "\n",
    "print(data.y[0]) # label\n",
    "print(data.y.unique())\n",
    "\n",
    "# print(data.edge_index[:,5]) # edge index\n",
    "print(data.train_masks) # train mask\n",
    "print(data.train_masks[0].shape)\n",
    "print(sum(data.train_masks[0]),sum(data.train_masks[6])) # train mask\n",
    "print(data.val_masks) # val mask\n",
    "print(data.test_masks) # test mask\n",
    "\n",
    "print(data.x[0].shape) # x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10858])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique edges 5278\n",
      "{1, 2}\n",
      "10556\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for i in range(data.edge_index.shape[1]):\n",
    "    curr = tuple(data.edge_index[:,i].numpy())\n",
    "    if curr in d:\n",
    "        d[curr] += 1\n",
    "    else:\n",
    "        d[curr] = 1\n",
    "unique_edges = set()\n",
    "for k in d.keys():\n",
    "    curr = tuple(sorted(list(k)))\n",
    "    unique_edges.add(curr)\n",
    "print(\"Number of unique edges\",len(unique_edges))\n",
    "print(set(d.values()))\n",
    "print(len(d))\n",
    "# most of the edges are repeated twice but it should be a directed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amangupt/anaconda3/envs/pgm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# Access the graph data\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(f\"preprocessed_data/new/cora_random_mistral.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 4096], raw_texts=[2708], category_names=[2708], entity=[2708]) <class 'torch_geometric.data.data.Data'>\n",
      "['Rule_Learning', 'Neural_Networks', 'Case_Based', 'Genetic_Algorithms', 'Theory', 'Reinforcement_Learning', 'Probabilistic_Methods']\n",
      " Stochastic pro-positionalization of non-determinate background knowledge. : It is a well-known fact that propositional learning algorithms require \"good\" features to perform well in practice. So a major step in data engineering for inductive learning is the construction of good features by domain experts. These features often represent properties of structured objects, where a property typically is the occurrence of a certain substructure having certain properties. To partly automate the process of \"feature engineering\", we devised an algorithm that searches for features which are defined by such substructures. The algorithm stochastically conducts a top-down search for first-order clauses, where each clause represents a binary feature. It differs from existing algorithms in that its search is not class-blind, and that it is capable of considering clauses (\"context\") of almost arbitrary length (size). Preliminary experiments are favorable, and support the view that this approach is promising.\n",
      "tensor(0)\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "[tensor([False,  True,  True,  ...,  True,  True,  True]), tensor([ True,  True,  True,  ...,  True, False,  True]), tensor([ True,  True, False,  ..., False,  True, False]), tensor([False,  True, False,  ...,  True,  True, False]), tensor([ True, False,  True,  ..., False, False,  True]), tensor([False,  True,  True,  ..., False,  True, False]), tensor([False,  True, False,  ..., False, False,  True]), tensor([ True, False,  True,  ...,  True,  True,  True]), tensor([ True,  True,  True,  ..., False, False,  True]), tensor([ True,  True, False,  ...,  True, False,  True])]\n",
      "torch.Size([2708])\n",
      "tensor(1624) tensor(1624)\n",
      "[tensor([ True, False, False,  ..., False, False, False]), tensor([False, False, False,  ..., False,  True, False]), tensor([False, False,  True,  ..., False, False,  True]), tensor([False, False, False,  ..., False, False, False]), tensor([False,  True, False,  ...,  True,  True, False]), tensor([False, False, False,  ..., False, False,  True]), tensor([False, False,  True,  ...,  True,  True, False]), tensor([False, False, False,  ..., False, False, False]), tensor([False, False, False,  ...,  True, False, False]), tensor([False, False, False,  ..., False,  True, False])]\n",
      "[tensor([False, False, False,  ..., False, False, False]), tensor([False, False, False,  ..., False, False, False]), tensor([False, False, False,  ...,  True, False, False]), tensor([ True, False,  True,  ..., False, False,  True]), tensor([False, False, False,  ..., False, False, False]), tensor([ True, False, False,  ...,  True, False, False]), tensor([ True, False, False,  ..., False, False, False]), tensor([False,  True, False,  ..., False, False, False]), tensor([False, False, False,  ..., False,  True, False]), tensor([False, False,  True,  ..., False, False, False])]\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "# analyse the data\n",
    "print(data, type(data))\n",
    "print(data.label_names)\n",
    "\n",
    "print(data.raw_text[0]) # raw text is title: abstract\n",
    "\n",
    "print(data.y[0]) # label\n",
    "print(data.y.unique())\n",
    "\n",
    "# print(data.edge_index[:,5]) # edge index\n",
    "print(data.train_masks) # train mask\n",
    "print(data.train_masks[0].shape)\n",
    "print(sum(data.train_masks[0]),sum(data.train_masks[6])) # train mask\n",
    "print(data.val_masks) # val mask\n",
    "print(data.test_masks) # test mask\n",
    "\n",
    "print(data.x[0].shape) # x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE FILE\n",
    "import torch\n",
    "data = torch.load(f\"preprocessed_data/new/cora_explanation.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks. \n",
      "\n",
      "Explanation: The paper describes a parallel language designed specifically for neural algorithms, with a focus on load balancing and irregular neural networks. The language is object-centered, with nodes and connections of a graph representing the neural network. The algorithms are based on parallel local computations and communication along the connections. Therefore, the paper is primarily related to the sub-category of AI known as Neural Networks.\n",
      "**********\n",
      "P: Neural Networks. \n",
      "**********\n",
      "E: Explanation: The paper describes a parallel language designed specifically for neural algorithms, with a focus on load balancing and irregular neural networks. The language is object-centered, with nodes and connections of a graph representing the neural network. The algorithms are based on parallel local computations and communication along the connections. Therefore, the paper is primarily related to the sub-category of AI known as Neural Networks.\n",
      "**************************************************\n",
      "Rule Learning, Theory. \n",
      "\n",
      "Explanation: \n",
      "The paper describes a method for finding the optimal parameter settings for a given learning algorithm using a particular dataset as training data. The method involves exploring the space of parameter values using best-first search and cross-validation, which is a form of rule learning. The paper also discusses the theoretical basis for the method and reports experimental results, indicating that it is effective in improving the performance of the learning algorithm.\n",
      "**********\n",
      "P: Rule Learning, Theory. \n",
      "**********\n",
      "E: Explanation: \n",
      "The paper describes a method for finding the optimal parameter settings for a given learning algorithm using a particular dataset as training data. The method involves exploring the space of parameter values using best-first search and cross-validation, which is a form of rule learning. The paper also discusses the theoretical basis for the method and reports experimental results, indicating that it is effective in improving the performance of the learning algorithm.\n",
      "**************************************************\n",
      "Genetic Algorithms, Neural Networks. \n",
      "\n",
      "Genetic Algorithms: The paper discusses the use of genetic algorithms in neuro-evolution, specifically in the context of evolving individual neurons and complete neural networks. It also presents a hierarchical approach to genetic search that overcomes the limitations of a purely neuron-based search. \n",
      "\n",
      "Neural Networks: The paper focuses on the evolution of neural networks, specifically exploring the benefits and limitations of evolving individual neurons versus complete networks. It also presents a hierarchical approach to neuro-evolution that integrates both neuron-level and network-level searches.\n",
      "**********\n",
      "P: Genetic Algorithms, Neural Networks. \n",
      "**********\n",
      "E: Genetic Algorithms: The paper discusses the use of genetic algorithms in neuro-evolution, specifically in the context of evolving individual neurons and complete neural networks. It also presents a hierarchical approach to genetic search that overcomes the limitations of a purely neuron-based search. \n",
      "\n",
      "Neural Networks: The paper focuses on the evolution of neural networks, specifically exploring the benefits and limitations of evolving individual neurons versus complete networks. It also presents a hierarchical approach to neuro-evolution that integrates both neuron-level and network-level searches.\n",
      "**************************************************\n",
      "Theory. \n",
      "\n",
      "Explanation: The paper presents a system, Forte, which refines first-order Horn-clause theories using a variety of different revision techniques. The focus is on improving an existing knowledge base using learning methods, which falls under the category of theory refinement. The paper does not mention any of the other sub-categories of AI listed in the question.\n",
      "**********\n",
      "P: Theory. \n",
      "**********\n",
      "E: Explanation: The paper presents a system, Forte, which refines first-order Horn-clause theories using a variety of different revision techniques. The focus is on improving an existing knowledge base using learning methods, which falls under the category of theory refinement. The paper does not mention any of the other sub-categories of AI listed in the question.\n",
      "**************************************************\n",
      "Neural Networks. \n",
      "\n",
      "Explanation: The paper focuses on the use of unsupervised lateral-inhibition neural networks for graphical inspection of multimodality. The three projection pursuit indices compared in the paper are all related to neural networks. Therefore, this paper belongs to the sub-category of AI known as Neural Networks.\n",
      "**********\n",
      "P: Neural Networks. \n",
      "**********\n",
      "E: Explanation: The paper focuses on the use of unsupervised lateral-inhibition neural networks for graphical inspection of multimodality. The three projection pursuit indices compared in the paper are all related to neural networks. Therefore, this paper belongs to the sub-category of AI known as Neural Networks.\n",
      "**************************************************\n",
      "Reinforcement Learning, Probabilistic Methods, Rule Learning\n",
      "\n",
      "Reinforcement Learning is present in the paper as the approach taken is to treat learning-strategy selection as a separate planning problem with its own set of goals, similar to the goal-management problems associated with traditional planning systems.\n",
      "\n",
      "Probabilistic Methods are present in the paper as the authors explore some issues, problems, and possible solutions in a framework where learning-strategy selection is treated as a separate planning problem with its own set of goals.\n",
      "\n",
      "Rule Learning is present in the paper as the authors present examples from a multistrategy learning system called Meta-AQUA, which is a rule-based system.\n",
      "**********\n",
      "P: Reinforcement Learning, Probabilistic Methods, Rule Learning\n",
      "**********\n",
      "E: Reinforcement Learning is present in the paper as the approach taken is to treat learning-strategy selection as a separate planning problem with its own set of goals, similar to the goal-management problems associated with traditional planning systems.\n",
      "\n",
      "Probabilistic Methods are present in the paper as the authors explore some issues, problems, and possible solutions in a framework where learning-strategy selection is treated as a separate planning problem with its own set of goals.\n",
      "\n",
      "Rule Learning is present in the paper as the authors present examples from a multistrategy learning system called Meta-AQUA, which is a rule-based system.\n",
      "**************************************************\n",
      "Neural Networks, Theory.\n",
      "\n",
      "Explanation: \n",
      "- Neural Networks: The paper discusses the use of recurrent networks as representations for formal language learning and the extraction of finite state machines from their internal state trajectories.\n",
      "- Theory: The paper presents two conditions that can lead to illusionary finite state descriptions, which is a theoretical analysis of the limitations of the extraction methods.\n",
      "**********\n",
      "P: Neural Networks, Theory.\n",
      "**********\n",
      "E: Explanation: \n",
      "- Neural Networks: The paper discusses the use of recurrent networks as representations for formal language learning and the extraction of finite state machines from their internal state trajectories.\n",
      "- Theory: The paper presents two conditions that can lead to illusionary finite state descriptions, which is a theoretical analysis of the limitations of the extraction methods.\n",
      "**************************************************\n",
      "Neural Networks, Control, Theory. \n",
      "\n",
      "Neural Networks: The paper's title explicitly mentions \"Neural Networks\" as one of the topics covered. The paper discusses various types of neural networks and their applications in control systems.\n",
      "\n",
      "Control: The paper focuses on the use of neural networks in control systems. It discusses how neural networks can be used to model and control complex systems, and provides examples of their use in various applications.\n",
      "\n",
      "Theory: The paper also discusses the theoretical foundations of neural networks and their applications in control systems. It covers topics such as backpropagation, gradient descent, and optimization algorithms, which are fundamental to the theory of neural networks.\n",
      "**********\n",
      "P: Neural Networks, Control, Theory. \n",
      "**********\n",
      "E: Neural Networks: The paper's title explicitly mentions \"Neural Networks\" as one of the topics covered. The paper discusses various types of neural networks and their applications in control systems.\n",
      "\n",
      "Control: The paper focuses on the use of neural networks in control systems. It discusses how neural networks can be used to model and control complex systems, and provides examples of their use in various applications.\n",
      "\n",
      "Theory: The paper also discusses the theoretical foundations of neural networks and their applications in control systems. It covers topics such as backpropagation, gradient descent, and optimization algorithms, which are fundamental to the theory of neural networks.\n",
      "**************************************************\n",
      "Probabilistic Methods. \n",
      "\n",
      "Explanation: The paper presents a Bayesian heuristic for finding the most probable hypothesis in a framework for learning from noisy data and fixed example size. The approach evaluates a hypothesis as a whole rather than one clause at a time, and the heuristic is incorporated in an ILP system called Lime. The paper also discusses the theoretical properties of the Bayesian approach and presents experimental results comparing Lime to other ILP systems.\n",
      "**********\n",
      "P: Probabilistic Methods. \n",
      "**********\n",
      "E: Explanation: The paper presents a Bayesian heuristic for finding the most probable hypothesis in a framework for learning from noisy data and fixed example size. The approach evaluates a hypothesis as a whole rather than one clause at a time, and the heuristic is incorporated in an ILP system called Lime. The paper also discusses the theoretical properties of the Bayesian approach and presents experimental results comparing Lime to other ILP systems.\n",
      "**************************************************\n",
      "Case Based\n",
      "\n",
      "Explanation: The paper discusses conversational case-based reasoning shells and the task of case engineering, which involves carefully authoring cases according to design guidelines to ensure good performance. The focus is on capturing knowledge as cases rather than rules, and incrementally extending the case library. The paper does not discuss genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, or theory.\n",
      "**********\n",
      "P: Case Based\n",
      "**********\n",
      "E: Explanation: The paper discusses conversational case-based reasoning shells and the task of case engineering, which involves carefully authoring cases according to design guidelines to ensure good performance. The focus is on capturing knowledge as cases rather than rules, and incrementally extending the case library. The paper does not discuss genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, or theory.\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# print 10 random examples from data\n",
    "for i in range(10):\n",
    "    idx = random.randint(0, len(data))\n",
    "    print(data[idx])\n",
    "    print(\"*\"*10)\n",
    "    print(\"P:\", data[idx].split(\"\\n\\n\")[0])\n",
    "    print(\"*\"*10)\n",
    "    print(\"E:\",\"\\n\\n\".join(data[idx].split(\"\\n\\n\")[1:]))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KEA File\n",
    "data = torch.load(f\"cora_entity.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# print 10 random examples from data\n",
    "for i in range(10):\n",
    "    idx = random.randint(0, len(data))\n",
    "    print(data[idx][0])\n",
    "    # print(\"*\"*10)\n",
    "    # print(\"P:\", data[idx].split(\"\\n\\n\")[0])\n",
    "    # print(\"*\"*10)\n",
    "    # print(\"E:\",\"\\n\\n\".join(data[idx].split(\"\\n\\n\")[1:]))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 4096], raw_texts=[2708], category_names=[2708], entity=[2708])\n",
      "MixedBread Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 1024], raw_texts=[2708], category_names=[2708], entity=[2708])\n"
     ]
    }
   ],
   "source": [
    "## Check model embeddings differences\n",
    "mistral_dataset = torch.load('/home/amangupt/Graph-LLM/preprocessed_data/new/cora_fixed_mistral.pt', map_location='cpu')\n",
    "print(\"Mistral\", mistral_dataset)\n",
    "\n",
    "mixedbread_dataset = torch.load('/home/amangupt/Graph-LLM/preprocessed_data/new/cora_fixed_mixedbread.pt', map_location='cpu')\n",
    "print(\"MixedBread\", mixedbread_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 384], raw_texts=[2708], category_names=[2708], entity=[2708])\n",
      "Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 384], raw_texts=[2708], category_names=[2708], entity=[2708])\n"
     ]
    }
   ],
   "source": [
    "kea_ta = torch.load('preprocessed_data/new/cora_fixed_KEA_TA_sbert.pt')\n",
    "print(kea_ta)\n",
    "kea = torch.load('preprocessed_data/new/cora_random_KEA_sbert.pt')\n",
    "print(kea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6203])\n"
     ]
    }
   ],
   "source": [
    "# choose 10 random index between 0 and len of dataset\n",
    "import random\n",
    "count = 0\n",
    "for j in range(1000):\n",
    "    idx = random.randint(0, len(mistral_dataset.raw_text))\n",
    "    # check cosine similarity between the embeddings at idx\n",
    "    import torch.nn.functional as F\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "\n",
    "    # mistral_emb = mistral_dataset.x[idx].unsqueeze(0)\n",
    "    # mixedbread_emb = mixedbread_dataset.x[idx].unsqueeze(0)\n",
    "    kea_ta_emb = kea_ta.x[idx].unsqueeze(0)\n",
    "    kea_emb = kea.x[idx].unsqueeze(0)\n",
    "\n",
    "    # print(\"KEA_TA vs KEA\", F.cosine_similarity(kea_ta_emb, kea_emb))\n",
    "    count += F.cosine_similarity(kea_ta_emb, kea_emb)\n",
    "\n",
    "print(count/1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
