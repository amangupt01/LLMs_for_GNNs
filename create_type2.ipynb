{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Text Level Datasets for (TAPE+KIA, TA + KEA, KEA, TAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "random_dataset = torch.load(f\"preprocessed_data/new/cora_random_sbert.pt\", map_location=\"cpu\")\n",
    "fixed_dataset = torch.load(f\"preprocessed_data/new/cora_fixed_sbert.pt\", map_location=\"cpu\")\n",
    "\n",
    "# PE \n",
    "pe_dataset = torch.load(f\"preprocessed_data/new/cora_explanation.pt\", map_location='cpu')\n",
    "\n",
    "# KEA\n",
    "kea_dataset = torch.load(f\"cora_entity.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 384], raw_texts=[2708], category_names=[2708])\n",
      "torch.Size([2708])\n",
      "tensor(1624)\n",
      "tensor(541)\n",
      "tensor(543)\n"
     ]
    }
   ],
   "source": [
    "print(random_dataset)\n",
    "print(random_dataset.train_masks[0].shape)\n",
    "print(random_dataset.train_masks[1].sum())\n",
    "print(random_dataset.val_masks[1].sum())\n",
    "print(random_dataset.test_masks[1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(raw_text=[2708], y=[2708], label_names=[7], edge_index=[2, 10858], train_masks=[10], val_masks=[10], test_masks=[10], x=[2708, 384], raw_texts=[2708], category_names=[2708])\n",
      "torch.Size([2708])\n",
      "tensor(140)\n",
      "tensor(500)\n",
      "tensor(2068)\n"
     ]
    }
   ],
   "source": [
    "print(fixed_dataset)\n",
    "print(fixed_dataset.train_masks[0].shape)\n",
    "print(fixed_dataset.train_masks[1].sum())\n",
    "print(fixed_dataset.val_masks[1].sum())\n",
    "print(fixed_dataset.test_masks[1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fixed_dataset.y == random_dataset.y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in: 0\n",
      "HGA: A Hardware-Based Genetic Algorithm: This paper presents the HGA, a genetic algorithm written in VHDL and intended for a hardware implementation. Due to pipelining, parallelization, and no function call overhead, a hardware GA yields a significant speedup over a software GA, which is especially useful when the GA is used for real-time applications, e.g. disk scheduling and image registration. Since a general-purpose GA requires that the fitness function be easily changed, the hardware implementation must exploit the reprogrammability of certain types of field-programmable gate arrays (FPGAs), which are programmed via a bit pattern stored in a static RAM and are thus easily reconfigured. After presenting some background on VHDL, this paper takes the reader through the HGA's code. We then describe some applications of the HGA that are feasible given the state-of-the-art in FPGA technology and summarize some possible extensions of the design. Finally, we review some other work in hardware-based GAs.\n",
      "**********\n",
      "Genetic Algorithms. \n",
      "\n",
      "Explanation: The paper discusses the implementation and performance analysis of a hardware-based genetic algorithm (HGA). It explains how genetic algorithms are a robust problem-solving method based on natural selection and how hardware's speed advantage and ability to parallelize offer great rewards to genetic algorithms. The paper also describes how the HGA was designed using VHDL to allow for easy scalability and act as a coprocessor with the CPU of a PC. Therefore, the paper primarily belongs to the sub-category of Genetic Algorithms in AI.\n",
      "**********\n",
      "HGA: A Hardware-Based Genetic Algorithm: This paper presents the HGA, a genetic algorithm written in VHDL and intended for a hardware implementation. Due to pipelining, parallelization, and no function call overhead, a hardware GA yields a significant speedup over a software GA, which is especially useful when the GA is used for real-time applications, e.g. disk scheduling and image registration. Since a general-purpose GA requires that the fitness function be easily changed, the hardware implementation must exploit the reprogrammability of certain types of field-programmable gate arrays (FPGAs), which are programmed via a bit pattern stored in a static RAM and are thus easily reconfigured. After presenting some background on VHDL, this paper takes the reader through the HGA's code. We then describe some applications of the HGA that are feasible given the state-of-the-art in FPGA technology and summarize some possible extensions of the design. Finally, we review some other work in hardware-based GAs.\n",
      "The above text contains the following entities: Genetic Algorithms. \n",
      "The following are the desciptions of each of the entities: Explanation: The paper discusses the implementation and performance analysis of a hardware-based genetic algorithm (HGA). It explains how genetic algorithms are a robust problem-solving method based on natural selection and how hardware's speed advantage and ability to parallelize offer great rewards to genetic algorithms. The paper also describes how the HGA was designed using VHDL to allow for easy scalability and act as a coprocessor with the CPU of a PC. Therefore, the paper primarily belongs to the sub-category of Genetic Algorithms in AI.\n"
     ]
    }
   ],
   "source": [
    "# create element wise addition of fixed_dataset and pe_dataset\n",
    "    # print(\"P:\", data[idx].split(\"\\n\\n\")[0])\n",
    "    # print(\"*\"*10)\n",
    "    # print(\"E:\",\"\\n\\n\".join(data[idx].split(\"\\n\\n\")[1:]))\n",
    "\n",
    "\n",
    "# p_dataset = [i.split(\"\\n\\n\")[0] for i in pe_dataset]\n",
    "# e_dataset = [\" \".join(i.split(\"\\n\\n\")[1:]) for i in pe_dataset]\n",
    "text = []\n",
    "count = 0\n",
    "for i in range(len(pe_dataset)):\n",
    "    try:\n",
    "        # p_dataset.append(i.split(\"\\n\\n\"))[0]\n",
    "        # e_dataset.append(\" \".join(i.split(\"\\n\\n\")[1:]))\n",
    "        text.append(fixed_dataset.raw_texts[i] +\n",
    "                    \"\\nThe above text contains the following labels: \" + pe_dataset[i].split(\"\\n\\n\")[0] +\n",
    "                    \"\\nThe following are the explainations for each of the labels: \" + \" \".join(pe_dataset[i].split(\"\\n\\n\")[1:]))\n",
    "    except:\n",
    "        count += 1\n",
    "        text.append(fixed_dataset.raw_texts[i]+ \n",
    "                    '\\nThe following is some information about the above text: ' + pe_dataset[i])\n",
    "print(\"Error in:\",count)\n",
    "\n",
    "\n",
    "idx = 98\n",
    "print(fixed_dataset.raw_text[idx])\n",
    "print(\"*\"*10)\n",
    "print(pe_dataset[idx])\n",
    "print(\"*\"*10)\n",
    "print(text[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
